{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "args = {\n",
        "    'model_path': 'grazh/llama-3.1-8b-clinical-es-inst-v3',\n",
        "    'task': 'text-generation',\n",
        "    'torch_dtype': torch.bfloat16,\n",
        "    'device_map': 'auto',\n",
        "    'do_sample': True,\n",
        "    'temperature': 0.1,\n",
        "    'max_new_tokens': 700,\n",
        "    'num_gpus': 1,\n",
        "    'load_8bit': False,\n",
        "    'cpu_offloading': False,\n",
        "    'max_gpu_memory': None,\n",
        "    'revision': 'main',\n",
        "    'debug': False,\n",
        "    'top_p': 1,\n",
        "    'full_texts_dir': '/content/drive/MyDrive/AI/MultiClinSum 2025/Data/test/multiclinsum_test_es',\n",
        "    'output_dir': '/content/drive/MyDrive/AI/MultiClinSum 2025/Results/LlamaGRPO_v3_test_set'\n",
        "}\n",
        "\n",
        "class DotDict:\n",
        "    def __init__(self, dictionary):\n",
        "        self._data = dictionary\n",
        "\n",
        "    def __getattr__(self, key):\n",
        "        if key in self._data:\n",
        "            return self._data[key]\n",
        "        else:\n",
        "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
        "\n",
        "    def __setattr__(self, key, value):\n",
        "        if key == '_data':\n",
        "            super().__setattr__(key, value)\n",
        "        else:\n",
        "            self._data[key] = value\n",
        "\n",
        "    def __delattr__(self, key):\n",
        "        if key in self._data:\n",
        "            del self._data[key]\n",
        "        else:\n",
        "            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n",
        "\n",
        "args = DotDict(args)"
      ],
      "metadata": {
        "id": "Nif2oWvqyvrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarization_pipeline = pipeline(\n",
        "    args.task,\n",
        "    model=args.model_path,\n",
        "    model_kwargs={\"torch_dtype\": args.torch_dtype},\n",
        "    device_map=args.device_map,\n",
        "    max_new_tokens=args.max_new_tokens,\n",
        "    do_sample=args.do_sample,\n",
        "    temperature=args.temperature,\n",
        "    top_p=args.top_p\n",
        ")"
      ],
      "metadata": {
        "id": "fAsxbrlny8G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_xml_summary(text: str) -> str:\n",
        "    answer = text.split(\"<summary>\")[-1]\n",
        "    answer = answer.split(\"</summary>\")[0]\n",
        "    return answer.strip()"
      ],
      "metadata": {
        "id": "yMywA1YCweDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Eres el asistente de inteligencia artificial de un médico. Se te proporciona el texto completo de un informe de caso clínico en español. Tu tarea es resumir el informe de caso clínico en español.\n",
        "Para ello, primero identifica y extrae las partes más importantes del informe completo, tales como la información clínica relevante, el diagnóstico, las intervenciones, los desenlaces y otros aspectos fundamentales del caso. Basándote en esta información, crea un resumen claro, preciso y coherente que incluya toda la información relevante del informe.\n",
        "Escribe el resumen final entre las etiquetas <summary> y </summary>.\n",
        "\n",
        "Respuesta:\n",
        "<summary>...</summary>\n",
        "\"\"\"\n",
        "\n",
        "def get_summary(text_to_summarize):\n",
        "  messages = [\n",
        "      {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "      {'role': 'user', 'content': f\"Por favor, resume el siguiente informe de caso clínico en español.\\n{text_to_summarize}\"}\n",
        "  ]\n",
        "\n",
        "  full_response = summarization_pipeline(messages)[0][\"generated_text\"][-1][\"content\"]\n",
        "  summary = extract_xml_summary(full_response)\n",
        "  return summary"
      ],
      "metadata": {
        "id": "iHaeKAxl0Ipb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_file_name(full_text_file_name):\n",
        "  return f\"{full_text_file_name.removesuffix('.txt')}_sum.txt\""
      ],
      "metadata": {
        "id": "AxXDs6Ku8Dx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "done = list(os.listdir(args.output_dir))\n",
        "\n",
        "full_text_file_names_sorted = sorted(\n",
        "    [f for f in os.listdir(args.full_texts_dir) if f.endswith(\".txt\")],\n",
        "    key=lambda x: int(x.split('_')[-2].removesuffix('.txt'))\n",
        ")\n",
        "\n",
        "print(\"File names\", full_text_file_names_sorted)\n",
        "\n",
        "for file_name in tqdm(full_text_file_names_sorted):\n",
        "  output_file_name = get_output_file_name(file_name)\n",
        "  if output_file_name in done:\n",
        "    print(\"Skipping\", file_name)\n",
        "    continue\n",
        "\n",
        "  print(\"Summarizing\", file_name, \"with output\", output_file_name)\n",
        "  with open(f\"{args.full_texts_dir}/{file_name}\", \"r\", encoding='utf8') as in_file:\n",
        "    with open(f\"{args.output_dir}/{output_file_name}\", \"w\", encoding='utf8') as out_file:\n",
        "      out_file.write(get_summary(in_file.read()))"
      ],
      "metadata": {
        "id": "MvakHp49aqHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "kW5qmKg4NLhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}